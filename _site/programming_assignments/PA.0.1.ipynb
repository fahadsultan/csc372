{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4878744",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>CSC-343 ARTIFICIAL INTELLIGENCE</h1>\n",
    "\n",
    "<h1>Programming Assignment PA.0.1.</h1>\n",
    "\n",
    "<h1>(Classical) Language Models</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0dd06",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4e1f9",
   "metadata": {},
   "source": [
    "<h3><i>Note: Importing external libraries is not permitted for this assignment</i></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900679de",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06da800",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52131d9b",
   "metadata": {},
   "source": [
    "Implement a function `tokenize` that takes in two **inputs**: \n",
    "\n",
    "1. `txt` (String): the text to tokenize\n",
    "2. `sep` (String): separator used for tokenization\n",
    "\n",
    "and returns as **output**:  \n",
    "\n",
    "* `tokens`: a **list** of **substrings** from `txt` _separated_ by `sep`\n",
    "\n",
    "For example, \n",
    "\n",
    "`txt` = `\"Natural Language Processing\"`<br/>\n",
    "`sep` = `\" \"` <br/>\n",
    "returns<br/> `tokens` = `[\"Natural\", \"Processing\", \"Processing\"]`\n",
    "\n",
    "a list of substrings in `txt` separated by `sep`\n",
    "\n",
    "See test cases for more examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf10d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt, sep):\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "assert tokenize(\"Syed Fahad Sultan\", \" \")         == [\"Syed\", \"Fahad\", \"Sultan\"],      \"Test case 1 failed\"\n",
    "assert tokenize(\"Apples, Oranges, Bananas\", \", \") == [\"Apples\", \"Oranges\", \"Bananas\"], \"Test case 2 failed\"\n",
    "assert tokenize(\"Semicolons; So Tricky\", \"; \")    == [\"Semicolons\", \"So Tricky\"],      \"Test case 3 failed\"\n",
    "assert tokenize(\" \", \" \")                         == ['', ''],                         \"Test case 4 failed\"\n",
    "assert tokenize(\"   \", \" \")                       == ['', '', '', ''],                 \"Test case 5 failed\"\n",
    "assert tokenize(\"\", \" \")                          == [''],                             \"Test case 6 failed\"\n",
    "\n",
    "print(\"All test cases passed successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bb446",
   "metadata": {},
   "source": [
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34886220",
   "metadata": {},
   "source": [
    "# 2. N-grams\n",
    "\n",
    "Implement a function `ngrams` which takes in two required inputs and one optional input: \n",
    "\n",
    "1. `txt` (String)  text for which to compute n-grams\n",
    "2. `n`   (Integer) number of adjacent tokens\n",
    "3. `sep` (String;  optional, default value is `\" \"`) separator for tokenization\n",
    "\n",
    "and returns a list `grams` containing lists of all `n` adjacent tokens in `txt`. Implement using a sliding window so `grams` preserves the order of the input `txt`. \n",
    "\n",
    "For example, \n",
    "\n",
    "`txt=\"Natural Language Processing\"`<br/>\n",
    "`n=2`<br/>\n",
    "`sep=\" \"`<br/>\n",
    "returns <br/>\n",
    "`[[\"Natural\", \"Language\"], [\"Language\", \"Processing\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72624cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(txt, n, sep=\" \"):\n",
    "    \n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ngrams(\"Syed Fahad Sultan\", 1)  == [[\"Syed\"], [\"Fahad\"], [\"Sultan\"]],        \"Test case 1 failed\"\n",
    "assert ngrams(\"Syed Fahad Sultan\", 2)  == [[\"Syed\", \"Fahad\"], [\"Fahad\", \"Sultan\"]], \"Test case 2 failed\"\n",
    "assert ngrams(\"Syed Fahad Sultan\", 3)  == [[\"Syed\", \"Fahad\", \"Sultan\"]],            \"Test case 3 failed\"\n",
    "\n",
    "assert ngrams(\"Apples, Oranges, Bananas\", 2, sep=\", \")\\\n",
    "                                       == [[\"Apples\", \"Oranges\"],\\\n",
    "                                           [\"Oranges\", \"Bananas\"]], \"Test case 4 failed\"\n",
    "\n",
    "assert ngrams(\"To be or not to be\", 3) == [['To', 'be', 'or'],\\\n",
    "                                            ['be', 'or', 'not'],\\\n",
    "                                            ['or', 'not', 'to'],\\\n",
    "                                            ['not', 'to', 'be']],   \"Test case 5 failed\"\n",
    "\n",
    "\n",
    "print(\"All test cases passed successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36326f92",
   "metadata": {},
   "source": [
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61044dc6",
   "metadata": {},
   "source": [
    "# 3. Standardized vocabulary\n",
    "\n",
    "Implement a function `standard_vocab` that takes as two inputs:\n",
    "\n",
    "1. `corpus` (List) list of strings\n",
    "2. `sep` (String; optional, default value: `\" \"`) separator for tokenization\n",
    "\n",
    "and returns a list `vocab` of all the unique tokens in `corpus` such that: \n",
    "\n",
    "1. the list `vocab` is alphabetically ordered\n",
    "2. each token in vocab is all lowercase\n",
    "\n",
    "For example, \n",
    "\n",
    "`corpus=[\"Furman is in Greenville\",\n",
    "    \"Greenville Tech is in Greenville\",\n",
    "    \"Bob Jones is in Greenville\"])`\n",
    "\n",
    "returns    \n",
    "\n",
    "`['bob', 'furman', 'greenville', 'in', 'is', 'jones', 'tech']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae188e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_vocab(corpus, sep=\" \"):\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fad561",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert standard_vocab(\\\n",
    "   [\"Furman is in Greenville\",\\\n",
    "    \"Greenville Tech is in Greenville\",\\\n",
    "    \"Bob Jones is in Greenville\"]) == ['bob',\\\n",
    "                                       'furman',\\\n",
    "                                       'greenville',\\\n",
    "                                       'in',\\\n",
    "                                       'is',\\\n",
    "                                       'jones',\\\n",
    "                                       'tech'],   \"Test case 1 failed\"\n",
    "\n",
    "assert standard_vocab(\\\n",
    "   [ \"An apparently affable and amiable aunt\",\\\n",
    "     \"Brilliant blue butterflies\",\\\n",
    "     \"Countless chicken clucked constantly\"]) \\\n",
    "                                    == ['affable',\\\n",
    "                                        'amiable',\\\n",
    "                                        'an',\\\n",
    "                                        'and',\\\n",
    "                                        'apparently',\\\n",
    "                                        'aunt',\\\n",
    "                                        'blue',\\\n",
    "                                        'brilliant',\\\n",
    "                                        'butterflies',\\\n",
    "                                        'chicken',\\\n",
    "                                        'clucked',\\\n",
    "                                        'constantly',\\\n",
    "                                        'countless'],   \"Test case 2 failed\"\n",
    "\n",
    "print(\"All test cases passed successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772956e",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd13e0",
   "metadata": {},
   "source": [
    "# 4. (Binary) Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab6aff",
   "metadata": {},
   "source": [
    "Implement a function `bbow` that takes in two **inputs**: \n",
    "\n",
    "1. `corpus` (List) list of strings\n",
    "2. `sep` (String; optional, default value: `\" \"`) separator for tokenization\n",
    "\n",
    "and returns two **outputs**: \n",
    "\n",
    "1. `vocab`  (list) the standardized vocabulary of `corpus`\n",
    "2. `matrix` (list) of length `len(corpus)` containing lists, each of length `len(vocab)` where element `matrix[i][j]` is 1 if input string `corpus[i]` contains token `vocab[j]` and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd91143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbow(txts, sep=' '):\n",
    "    \n",
    "    return vocab, count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bbow(\n",
    " [\"Furman is in Greenville\",\\\n",
    "  \"Greenville Tech is in Greenville\",\\\n",
    "  \"Bob Jones is in Greenville\"]) == (['bob', 'furman', 'greenville', 'in', 'is', 'jones', 'tech'], \\\n",
    "                                    [[ 0,     1,        1,            1,     1,    0,       0],\\\n",
    "                                     [ 0,     0,        1,            1,     1,    0,       1],\\\n",
    "                                     [ 1,     0,        1,            1,     1,    1,       0]]),\\\n",
    "  \"Test case 1 failed\"\n",
    "                                     \n",
    "assert bbow(\n",
    " [ \"An apparently affable and amiable aunt\",\\\n",
    "   \"Brilliant blue butterflies\",\\\n",
    "   \"Countless chicken clucked constantly\"]) == (['affable','amiable','an','and','apparently','aunt',\\\n",
    "                                                 'blue','brilliant','butterflies',\\\n",
    "                                                 'chicken','clucked','constantly','countless'],\\\n",
    "                                                [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                                 [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
    "                                                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]), \\\n",
    "  \"Test case 2 failed\"\n",
    "                                     \n",
    "print(\"All test cases passed successfully :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec34a5",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
